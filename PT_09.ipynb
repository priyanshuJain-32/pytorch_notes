{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial 09 - Dataset and DataLoader - Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In batch training we define:\n",
    "\n",
    "1. EPOCH = 1 forward and backward pass of all training samples\n",
    "\n",
    "2. BATCH_SIZE = number of training samples in one forward & backward pass\n",
    "\n",
    "3. Number of iterations = number of passes, each pass using [batch_size] number of samples\n",
    "\n",
    "e.g.  100 samples, BATCH_SIZE = 20 --> 100/20 = 5 iterations for each EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and DataLoader\n",
    "\n",
    "Using these two classes makes loading data easier in batches for the further process.\n",
    "\n",
    "Here we create our own class and subclass Dataset class and implement \n",
    "- \\__init__, \n",
    "- \\__getitem__, and \n",
    "- \\__len__ methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataset class for loading data\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # super(WineDataset, self).__init__() # calling this crashes kernel\n",
    "\n",
    "        xy = np.loadtxt(\"./data/wine/wine.csv\", delimiter=\",\", dtype=np.float32, skiprows = 1) # skip first header\n",
    "        self.x = torch.from_numpy(xy[:,1:]) # converted to tensor\n",
    "        self.y = torch.from_numpy(xy[:,[0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index] # returns tuple of data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples # return len samples\n",
    "    \n",
    "data = WineDataset() # create WineDataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] # checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create dataloader object through which we will load data in batches for training\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset=data, batch_size=BATCH_SIZE, shuffle=True) # num_workers was crashing kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.1820e+01, 1.4700e+00, 1.9900e+00, 2.0800e+01, 8.6000e+01, 1.9800e+00,\n",
       "          1.6000e+00, 3.0000e-01, 1.5300e+00, 1.9500e+00, 9.5000e-01, 3.3300e+00,\n",
       "          4.9500e+02],\n",
       "         [1.2290e+01, 1.6100e+00, 2.2100e+00, 2.0400e+01, 1.0300e+02, 1.1000e+00,\n",
       "          1.0200e+00, 3.7000e-01, 1.4600e+00, 3.0500e+00, 9.0600e-01, 1.8200e+00,\n",
       "          8.7000e+02],\n",
       "         [1.4370e+01, 1.9500e+00, 2.5000e+00, 1.6800e+01, 1.1300e+02, 3.8500e+00,\n",
       "          3.4900e+00, 2.4000e-01, 2.1800e+00, 7.8000e+00, 8.6000e-01, 3.4500e+00,\n",
       "          1.4800e+03],\n",
       "         [1.1660e+01, 1.8800e+00, 1.9200e+00, 1.6000e+01, 9.7000e+01, 1.6100e+00,\n",
       "          1.5700e+00, 3.4000e-01, 1.1500e+00, 3.8000e+00, 1.2300e+00, 2.1400e+00,\n",
       "          4.2800e+02]]),\n",
       " tensor([[2.],\n",
       "         [2.],\n",
       "         [1.],\n",
       "         [2.]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataiter.__next__() # checking iterator\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 2\n",
    "TOTAL_SAMPLES = len(data)\n",
    "N_ITERATIONS = math.ceil(TOTAL_SAMPLES/BATCH_SIZE)\n",
    "\n",
    "print(TOTAL_SAMPLES, N_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/2, STEP 5/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 10/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 15/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 20/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 25/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 30/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 35/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 40/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 1/2, STEP 45/45, INPUTS torch.Size([2, 13])\n",
      "EPOCH 2/2, STEP 5/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 10/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 15/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 20/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 25/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 30/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 35/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 40/45, INPUTS torch.Size([4, 13])\n",
      "EPOCH 2/2, STEP 45/45, INPUTS torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if (i+1)%5 == 0:\n",
    "            print(\"EPOCH {}/{}, STEP {}/{}, INPUTS {}\".format(epoch+1, NUM_EPOCHS, i+1, N_ITERATIONS, inputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:49<00:00, 199955.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 60812.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 658063.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1376085.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some famous datasets we can get built-in\n",
    "torchvision.datasets.MNIST(root = \"./data\", transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# fashion dataset etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
